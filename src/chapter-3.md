# 深層学習とは？

TensorFlowの環境ができたから、さっそくプログラミング……の前に、深層学習とはどんなものなのか、その概要を勉強することにしましょう。理解していた方が、深層学習をより良く使えますもんね。

## 形式ニューロン

まずは、基礎から。深層学習はニューラル・ネットワークを学習させるテクニックなのですが、そのニューラル・ネットワークを構成する要素であるニューロンの話から始めさせてください。

生物のニューロンは、その細胞体から何本も生えた樹状突起で刺激を受け取って、なんだかよくわからない基準でいい感じに刺激群を判断して、軸索を通じて次のニューロンに刺激を送ったり送らなかったりします。こんなので、我々の脳は構成されているみたい。

＊＊＊絵＊＊＊

で、このニューロンの「なんだかよくわからない基準でいい感じ」の部分を、はるか昔の1943年にマッカロックさんとピッツさんが形式ニューロンとしてソフトウェア化してくれました。図にすると、以下のような感じ。

＊＊＊絵＊＊＊

Pythonのコードにすると、以下のような感じ。

```python
weights = (4, 2, 2)  # 重み
threshold = 3  # しきい値


def formal_neuron(*xs):  # xsは3つの真偽値
    # 真である場合の重みを足しあわせて、しきい値を超えたら真。そうでなければ偽。
    return sum(weights[i] for i, x in enumerate(xs) if x) > threshold
```

この形式ニューロン、こんな単純な構造のくせに、なかなかいい感じに判断してくれます。

具体的にいきましょう。先ほどのコードの`formal_neuron`に、私が今週末に遊びに行くかどうかを表現させてみます。引数である`xs`を構成する3つの真偽値は、それぞれ「晴れている」、「読むべき本がない」、「やらなければならない仕事がない」とします（否定形でわかりづらくてごめんなさい。マイナスの数字を使うのは難しかったの……）。

私の趣味はオートバイで、しかも独身で家族いなくてコミュ症で友達もいません。だから、晴れていれば（他の条件がどうであっても）オートバイでツーリングに行きます。確認してみましょう。

```python
>>> formal_neuron(True, False, False)  # 晴れ、本あり、仕事あり。
True  # 遊びに行く
>>>
```

私は、読むべき有益な本があっても仕事が残っていても、晴れていればオートバイで出かけちゃうる阿呆だということがわかりました。なんか、光に反応する昆虫みたいですな。

でも、雨が降ったらどうなるのでしょうか？　続けて試してみましょう。

```python
>>> formal_neuron(False, True, False)  # 雨、本なし、仕事あり。
False  # 遊びに行かない。
>>> formal_neuron(False, False, True)  # 雨、本あり、仕事なし。
False  # 遊びに行かない。
>>> formal_neuron(False, True, True)   # 雨、本なし、仕事なし。
True   # 遊びに行く。
>>> 
```

なるほど、まだ読んでいない本があるならそれを読む、仕事があるなら仕事をする、どちらもないなら、雨であってもカッパを着てオートバイに乗るわけですね。こんな単純なコードなのに、いい感じに判断できていて面白いでしょ？

でも、もっと面白いことがあるんです。重みである`weights`としきい値である`threshold`を調整するだけで、私とは異なる人の場合も表現できるんですよ。たとえば、しきい値を`4`にすれば（`threshold = 4`）、晴れていれば遊びに行くけど雨ならば遊びに行かないという、雨が嫌いなオートバイ大好き人間になります。晴れの重みを減らして本の重みを増やしたなら（`weights = [2, 4, 2]`）、ビブリオマニアの読子＝リードマンさんですね。

その他、ANDとOR、NOTなんてのも表現できます。形式ニューロンを多段に組み合わせて良いのであれば、XORも表現できます。

```python
from itertools import chain


def and_(*xs):
    # weightsとthresholdは、関数内に埋め込んでいます。
    return sum((1, 1)[i] for i, x in enumerate(xs) if x) > 1.5


def or_(*xs):
    return sum((1, 1)[i] for i, x in enumerate(xs) if x) > 0.5


def not_(*xs):
    return sum((-1,)[i] for i, x in enumerate(xs) if x) > -0.5


def xor_(*xs):
    def next_layer(*next_xs):
        return sum((1, 1, -2)[i] for i, x in enumerate(next_xs) if x) > 0.5

    return next_layer(*chain(xs, ((sum((1, 1)[i] for i, x in enumerate(xs) if x) > 1.5),)))
```

スゲー。ANDとORとNOTとXORを表現できるってことは、つまり、どんな電子回路でも作れるってことです。CPUを作れちゃうってことなんですよ！

これなら、もっとスゴイこともできそうです。適切な重みとしきい値を持った形式ニューロンをいい感じに積み重ねていけば、生物の脳と同様の判断、たとえば網膜に映った画像から餌の有無を判断できるかもしれません。だって、形式ニューロンは生物のニューロンを模しているんですから。

## 単純パーセプトロン

……残念なことに、そうは問屋が卸してくれませんでした。だって、パラメーター設定が難しすぎますもん。前節のXORのコードを書けたのは、Wikipediaの形式ニューロンの説明の中にパラメーターが載っていたからで、私が頑張ってパラメーターを設定したからではありません。XOR程度でこれなのですから、画像認識のためのパラメーター調整なんてのは、人間がやれる作業じゃあないですよ。

では、どうするか？　人間ができないなら、機械にやらせてしまえばよいでしょう。そのために、1957年にローゼンブラットさんがパーセプトロンを発明しました。

パーセプトロンをコードにすると、以下のようになります。形式ニューロンとの違いは、入力値の要素と重みの要素を掛け算するようになったのと、しきい値（`threshold`）がバイアス（`bias`）と改名されて左に移動した程度。あと、掛け算をするので、入力値は実数になるところ。機械学習するんだからパラメーターは私がやらなくても良いということで、重みもバイアスも適当な値にしてあります。

```python
weights = (0, 1)  # 重み
bias = -0.5  # バイアス


def perceptron(*xs):
    return 1 if sum(x * weight for x, weight in zip(xs, weights)) + bias >= 0 else 0
```

パーセプトロンの良い所は、重みやバイアスを調整しやすいところです。

形式ニューロンだと、判断に間違えた時に、重みやバイアスをどの程度変更したらよいのかがわからなかいですよね。でも、パーセプトロンなら、`x * weights`になっていますから、`x`に関係した数値で`weight`を増減させてよげればよさそうです。`bias`については、`x`が1だとみなしちゃえばよいでしょう。

というわけで、このパーセプトロンを機械学習させます。今回は、男の子ならみんな大好きスーパー・カーのオートバイ版であるスーパー・スポーツ（とにかく速くて馬鹿っぽいオートバイだと思ってください）の判別としましょう。上のコードでは重みが2個なので`xs`の数は2つ、オートバイを表現する数値2つということで、エンジンの大きさ（排気量）を価格を入力にします。

さっそく、日本のバイクメーカーであるホンダとヤマハとカワサキとスズキのスーパー・スポーツなオートバイのデータを集めてきました。スーパー・スポーツでは「ない」オートバイも無いと判断になりませんので、2016年の販売台数トップ10（400cc以上）の中から国産のバイクを抜き出したものも付け加えます。あと、もちろん機械学習して学習結果を検証するコードが必要で、パーセプトロンの機械学習がどのように進むのかを可視化するコードもあったほうがよいでしょう。

というわけで、以下のコードを作りました（長くてごめんなさい。TensorFlowを使いだせば、コードは短くなりますのでご安心を）。

```python
import matplotlib.pyplot as plot
import matplotlib.animation as animation

from itertools import starmap
from random import shuffle


weights = (0, 1)  # 重み
bias = -0.5  # バイアス


# パーセプトロン。
def perceptron(*xs):
    # 入力と重みを掛け算して足しあわせてバイアスを足した結果が0以上なら1、そうでなければ0を返します。
    return 1 if sum(x * weight for x, weight in zip(xs, weights)) + bias >= 0 else 0


# 機械学習用のデータ。
motorcycles = (
    # ひとつ目の要素が1なら、スーパー・スポーツ。ふたつ目の要素は、排気量と価格のタプル。
    (1, ( 999, 1501200)),  # ホンダ CBR1000RR（ロスホワイト）
    (1, ( 999, 1468800)),  # ホンダ CBR1000RR
    (1, ( 999, 1674000)),  # ホンダ CBR1000RR<ABS>（ロスホワイト）
    (1, ( 999, 2030400)),  # ホンダ CBR1000RR SP
    (1, ( 599, 1334880)),  # ホンダ CBR600RR<ABS>（ロスホワイト）
    (1, ( 599, 1162080)),  # ホンダ CBR600RR（ロスホワイト）
    (1, ( 599, 1302480)),  # ホンダ CBR600RR<ABS>（グラファイトブラック）
    (1, ( 599, 1129680)),  # ホンダ CBR600RR（グラファイトブラック）
    (1, ( 998, 2430000)),  # ヤマハ YZF-R1（ライトれディッシュイエローソリット1）
    (1, ( 998, 2376000)),  # ヤマハ YZF-R1（ディープパープリッシュブルーメタリックC）
    (1, ( 998, 3186000)),  # ヤマハ YZF-R1M
    (1, ( 998, 1782000)),  # カワサキ Ninja ZX-10R
    (1, ( 599,  924480)),  # カワサキ Ninja ZX-6R
    (1, ( 999, 1695600)),  # スズキ GSX-R1000
    (1, ( 999, 1760400)),  # スズキ GSX-R1000 ABS
    (1, ( 750, 1544400)),  # スズキ GSX-R750
    (1, ( 599, 1425600)),  # スズキ GSX-R600

    # ひとつ目の要素が0なら、スーパースポーツではない。
    (0, ( 845, 1069200)),  # ヤマハ MT-09 TRACER ABS
    (0, ( 745,  743040)),  # ホンダ NC750X
    (0, ( 745,  793800)),  # ホンダ NC750X<ABS>
    (0, ( 745,  859680)),  # ホンダ NC750X DCS<ABS>
    (0, ( 745,  924480)),  # ホンダ NC750X DCS<ABS> E Package
    (0, ( 845, 1004400)),  # ヤマハ MT-09 ABS
    (0, ( 998, 1382400)),  # ホンダ CRF1000L（ヴィクトリーレッド、パールグレアホワイト）
    (0, ( 998, 1350000)),  # ホンダ CRF1000L（キャンディープロミネンスレッド、デジタルシルバーメタリック）
    (0, ( 998, 1490400)),  # ホンダ CRF1000L DCS（ヴィクトリーレッド、パールグレアホワイト）
    (0, ( 998, 1458000)),  # ホンダ CRF1000L DCS（キャンディープロミネンスレッド、デジタルシルバーメタリック）
    (0, ( 998, 1115640)),  # スズキ GSX-S1000 ABS
    (0, ( 998, 1166400)),  # スズキ GSX-S1000F ABS
    (0, ( 688,  760320)),  # ヤマハ MT-07 ABS
    (0, ( 688,  710640)),  # ヤマハ MT-07
    (0, ( 845, 1042200)),  # ヤマハ XSR900
    (0, (1164, 1172880)))  # カワサキ ZRX1200 DAEG


# 機械学習します。
def train():
    global weights, bias

    # 排気量と価格を、0〜1の間の数値に正規化します。
    min_xs = tuple(min(starmap(lambda _, xs: xs[i], motorcycles)) for i in range(2))
    max_xs = tuple(max(starmap(lambda _, xs: xs[i], motorcycles)) for i in range(2))
    normalized_motorcycles = [(label, tuple((xs[i] - min_xs[i]) / (max_xs[i] - min_xs[i]) for i in range(2))) for label, xs in motorcycles]

    # 検証用データ（test_data）と学習用データ（train_data）に分けます。
    shuffle(normalized_motorcycles)
    test_data, train_data = normalized_motorcycles[:5], normalized_motorcycles[5:]

    # 学習率（詳細は本文を参照してください）。
    learning_rate = 0.01

    # Matplotlibを使用して、可視化します。
    # アニメーション用の変数。
    figure = plot.figure()
    images = []
    
    # データを散布図として描画します。
    plot.plot([xs[0] for label, xs in train_data if label == 0],
              [xs[1] for label, xs in train_data if label == 0],
              'bo',
              marker='.')
    plot.plot([xs[0] for label, xs in train_data if label == 1],
              [xs[1] for label, xs in train_data if label == 1],
              'ro',
              marker='.')
    plot.plot([xs[0] for label, xs in test_data if label == 0],
              [xs[1] for label, xs in test_data if label == 0],
              'bo',
              marker='+')
    plot.plot([xs[0] for label, xs in test_data if label == 1],
              [xs[1] for label, xs in test_data if label == 1],
              'ro',
              marker='+')

    # 学習。
    for i in range(100):
        images.append(plot.plot([-(weights[0] / weights[1]) * i - (bias / weights[1]) for i in range(2)], 'g'))

        for label, xs in train_data:
            result = perceptron(*xs)
            if (result != label):
                weights = tuple(weights[i] + learning_rate * (label - result) * xs[i] for i in range(2))
                bias = bias + learning_rate * (label - result)

    # 検証。
    for label, xs in test_data:
        print("{0}: {1}".format(label, perceptron(*xs)))

    # 学習過程のアニメーションを表示します。
    artist_animation = animation.ArtistAnimation(figure, images, interval=1, repeat_delay=1000)
    # artist_animation.save('perceptron.gif', writer='imagemagick')
    plot.show()


if __name__ == '__main__':
    train()
```

機械学習させるコードは、以下の部分です。

```python
for label, xs in train_data:
    result = perceptron(*xs)

    # もし答えを間違えたら、重みとバイアスを調整します。
    if (result != label):
        weights = tuple(weights[i] + learning_rate * (label - result) * xs[i] for i in range(2))
        bias = bias + learning_rate * (label - result)
```

重みの要素（`weights[i]`）を、対応する入力の要素（`xs[i]`）で増減（正解が`1`で答えが`0`なら`1 - 0 = 1`を掛けているので増、逆なら`0 - 1 = -1`を掛けるので減になります）させています。で、対応する入力の要素そのままだと重みやバイアスが大きく変わりすぎてしまいますから、学習率（`learning_rate`）に設定した適切な値（今回は`0.01`。適当に決めました）を掛け算しています。あと、データ数が少ないので、上の学習を100回繰り返しています。

こんな単純な処理ですけど、パーセプトロンはうまいこと機械学習してくれます。プログラムを実行すると、最終的に以下の検証コード

```python
for label, xs in test_data:
    # 「<正解データ>: <パーセプトロンの解答>」を表示します。
    print("{0}: {1}".format(label, perceptron(*xs)))
```

が実行されて、コンソールに以下のような内容が表示されます。

```shell
0: 0
0: 0
1: 1
1: 1
0: 0
```

コロンの左が正解（スーパー・スポーツなら1、そうでなければ0）で、右がパーセプトロンが出した解答ですから、やりました、全問正解です（検証データの選択はランダムなので違う結果になることもありますけど、概ね正解できるはず）。

おおすげーと思っていると、続いて、別ウィンドウが開いて、以下の画像のようなアニメーションが表示されるはずです。

![パーセプトロン](images/perceptron.gif)

X軸を排気量、Y軸を価格に（0〜1の数値になるように正規化しています）、スーパー・スポーツを赤、そうでない場合を青で描画しています。で、このウネウネと動いている緑の直線が、重みとバイアスを使用した判断の境目です。

……えっと、単純なパーセプトロンだと、データを直線で区切ることしかできないんですよ（重みや入力が3つの場合は、3次元空間上の2次元平面で区切ります）。今回うまく学習できたのは、たまたまデータがパーセプトロンに合うものだったからに過ぎません。たとえば、体重と身長から長生きするかを判断する（身長が低い方が長生きする傾向にあるので直線で区切れそうに思えるけど、痩せすぎも太りすぎも長生きしそうにないので直線では区切れない）ような場合はダメです。複雑な判断の場合には、この単純なパーセプトロンは使用できないんです。

＊＊＊身長と体重と長生きの図＊＊＊

## 多層パーセプトロン

だったら、パーセプトロンを組み合わせればよいのでは？　具体的には、以下の図のように積み重ねちゃえばよいのでは？

＊＊＊パーセプトロンを積み重ねた図＊＊＊

それはもちろんその通りなのですけど、ただ積み重ねるだけだと、単純なパーセプトロンの出力は0か1のどちらかであるという点が問題となります。重みやバイアスを調整しても出力が0のまま変わらなかったり、ほんの少し調整したら出力が0からいきなり1に変わったりするわけですから、この出力を受け取る上の層のパーセプトロンはとても混乱してしまい、学習が不可能になります。そもそも、パーセプトロンへの入力は実数なのですから、パーセプトロンからの出力が0または1だったら積み重ねようがありませんしね。

というわけで、パーセプトロンの出力を実数にしてみます。以下のような感じでしょうか？

```python
def perceptron(*xs):
    return sum(x * weight for x, weight in zip(xs, weights)) + bias
```

残念！　これだけではまだダメです。これだけだと、層を増やしても能力が上がらないらしいんですよ（私は数学できないので、その理由は分からなかったけど）。どこかに非線形な要素を追加しないとダメなんだってさ。

だから、「いきなり変わるのではなくてなだらかに変わる（そうじゃないと、学習できない）」かつ「非線形（そうじゃないと、複雑な分類ができない）」な、活性化関数と呼ばれる関数でくるんであげましょう。大昔からある活性化関数としてシグモイド関数というのがあって、その出力は以下のような形をしています。

＊＊＊シグモイド関数の図＊＊＊

うん、なだらかかつ非線形ですな。ギャップがなくて直線じゃあないですもん。

あと、活性化関数にはもう一つ特徴が必要で、それは「微分できる」ことなんです。重みやバイアスをどの程度変えたらどのように出力が変わるのかが微分で分かるみたいで、そうすれば積み重ねたニューロンを、上の層から下の層に、出力から入力へと逆に調整していくことが可能になるんです。このような、微分をうまいこと使って誤差を逆向きに伝播させて学習させる方式を、逆誤差伝播学習法（バックプロパゲーション）と呼びます。

というわけで、出力を実数にして活性化関数を間に挟んで、逆誤差伝播学習法に類する方法で複数層のニューロンを学習させるという方式が、1960年代という大昔に発明されました。ただ、当時のコンピューターは貧弱すぎて、逆誤差伝播学習法はまともに動かなかったらしいんですよ。使えないものだからみんなすぐに忘れ、また誰かが再発明してまたみんなに忘れられ……という可哀想な運命を辿った挙句、1986年にラメルハートさんとその仲間たちが逆誤差伝播学習法と名づけたあたりで、コンピューターの性能が追いついてやっと定着したみたい。

ただね、逆誤差伝播学習法で何層にも積み重ねたニューロンを学習する場合、シグモイド関数ってのは特徴が消えちゃいやすいんですよ。シグモイド関数の出力をシグモイド関数の入力にすると、最初よりも大分なだらかになっちゃう。

＊＊＊多段のシグモイド関数の図＊＊＊

この問題を、勾配消失問題と呼びます。というわけで、今ではシグモイド関数はあまり使われていなくて、ReLUという以下の図のような特徴を持つ活性化関数が流行っています。

＊＊＊ReLUの図＊＊＊

この図は、先程のシグモイド関数の場合とは全然似ていませんけどね……。でも、これでも大丈夫なんです。線はつながっているので、なだらかとは言っても問題はありません。0のところで曲がっていますから、線形ではありません。0のところは明らかに微分できませんけど、0か1のどちらかに決め打っちゃえば（これを劣微分と言います）大丈夫です。ほら、なんとかなりました。

実際、ReLUはとても優れた活性化関数です。何度呼び出しても平らになりませんから、勾配消失は発生しづらくなります。単純なので、計算が速いです。あと、何といってもコードが簡単です。だから、ReLUという活性化関数を組み込んだパーセプトロンのコードを、ここですぐ紹介できます。

```python
def perceptron(*xs):
    return max(0, sum(x * weight for x, weight in zip(xs, weights)) + bias)
```

はい。`max(0, ...)`の部分を追加しただけです。

でもこれで、逆誤差伝播学習法で学習させられる「多層パーセプトロン」が完成しました（対比のために、前節で述べたようなパーセプトロンは「単純パーセプトロン」と呼ばれます）。

実は、深層学習は、この多層パーセプロトンを逆誤差伝播学習法で学習させるところから始まっています。1986年は、ファミコン用のドラゴンクエストIが発売された年。Play Station 4でファイナル・ファンタジーXVをやっている我々から見たら、旧世代のカンタンな技術なんですよ。

## TensorFlowは、計算グラフを自動微分して逆誤差伝播学習法で機械学習する環境である

でも、深層学習するには微分をしなければなりません……。みなさんはどうか分かりませんけど、私は絶対に微分したくないです。そもそも、私立文系の私に微分はできないし！　だから、そろそろTensorFlowを使ってみましょう。TensorFlowは自動微分の機能を持っていますから、私のような数学が全くできない人間でも深層学習できるんですよ[^4]。



## 畳み込みニューラルネットワーク



[^4]: Chainerは、微分をライブラリ内に実装することでこの問題を解決しています。本稿で述べる範囲のニューラル・ネットワークを作る場合は、やっぱり微分は無関係で深層学習できます。ご安心を。
[^5]: 最近の再帰ニューラル・ネットワークとかだと、どこがどうニューラルなのかわからない感じになっていますけど……。
